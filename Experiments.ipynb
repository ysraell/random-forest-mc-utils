{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template \n",
    "\n",
    "- Author: Israel Oliveira [\\[e-mail\\]](mailto:'Israel%20Oliveira%20'<prof.israel@gmail.com>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T01:43:38.175603Z",
     "iopub.status.busy": "2021-09-16T01:43:38.175252Z",
     "iopub.status.idle": "2021-09-16T01:43:42.045412Z",
     "shell.execute_reply": "2021-09-16T01:43:42.044798Z",
     "shell.execute_reply.started": "2021-09-16T01:43:38.175506Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting random-forest-mc\n",
      "  Downloading random_forest_mc-0.3.3-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.9/site-packages (from random-forest-mc) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/site-packages (from random-forest-mc) (1.20.3)\n",
      "Requirement already satisfied: tqdm>=4.60 in /usr/local/lib/python3.9/site-packages (from random-forest-mc) (4.62.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas>=1.3->random-forest-mc) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas>=1.3->random-forest-mc) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas>=1.3->random-forest-mc) (1.16.0)\n",
      "Installing collected packages: random-forest-mc\n",
      "Successfully installed random-forest-mc-0.3.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U random-forest-mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T01:43:42.047332Z",
     "iopub.status.busy": "2021-09-16T01:43:42.047057Z",
     "iopub.status.idle": "2021-09-16T01:43:42.367677Z",
     "shell.execute_reply": "2021-09-16T01:43:42.365746Z",
     "shell.execute_reply.started": "2021-09-16T01:43:42.047295Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T01:43:42.371077Z",
     "iopub.status.busy": "2021-09-16T01:43:42.370465Z",
     "iopub.status.idle": "2021-09-16T01:43:44.375552Z",
     "shell.execute_reply": "2021-09-16T01:43:44.375001Z",
     "shell.execute_reply.started": "2021-09-16T01:43:42.370991Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random_forest_mc.model import RandomForestMC\n",
    "from random_forest_mc.utils import load_file_json, dump_file_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from collections import Counter\n",
    "import hashlib\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T01:43:44.377196Z",
     "iopub.status.busy": "2021-09-16T01:43:44.376966Z",
     "iopub.status.idle": "2021-09-16T01:43:44.382602Z",
     "shell.execute_reply": "2021-09-16T01:43:44.382023Z",
     "shell.execute_reply.started": "2021-09-16T01:43:44.377171Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# from glob import glob\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from matplotlib import rcParams\n",
    "# from cycler import cycler\n",
    "\n",
    "# rcParams['figure.figsize'] = 12, 8 # 18, 5\n",
    "# rcParams['axes.spines.top'] = False\n",
    "# rcParams['axes.spines.right'] = False\n",
    "# rcParams['axes.grid'] = True\n",
    "# rcParams['axes.prop_cycle'] = cycler(color=['#365977'])\n",
    "# rcParams['lines.linewidth'] = 2.5\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.set_theme()\n",
    "\n",
    "# pd.set_option(\"max_columns\", None)\n",
    "# pd.set_option(\"max_rows\", None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def md(arg):\n",
    "    display(Markdown(arg))\n",
    "\n",
    "# from pandas_profiling import ProfileReport\n",
    "# #report = ProfileReport(#DataFrame here#, minimal=True)\n",
    "# #report.to\n",
    "\n",
    "# import pyarrow.parquet as pq\n",
    "# #df = pq.ParquetDataset(path_to_folder_with_parquets, filesystem=None).read_pandas().to_pandas()\n",
    "\n",
    "# import json\n",
    "# def open_file_json(path,mode='r',var=None):\n",
    "#     if mode == 'w':\n",
    "#         with open(path,'w') as f:\n",
    "#             json.dump(var, f)\n",
    "#     if mode == 'r':\n",
    "#         with open(path,'r') as f:\n",
    "#             return json.load(f)\n",
    "\n",
    "# import functools\n",
    "# import operator\n",
    "# def flat(a):\n",
    "#     return functools.reduce(operator.iconcat, a, [])\n",
    "\n",
    "# import json\n",
    "# from glob import glob\n",
    "# from typing import NewType\n",
    "\n",
    "\n",
    "# DictsPathType = NewType(\"DictsPath\", str)\n",
    "\n",
    "\n",
    "# def open_file_json(path):\n",
    "#     with open(path, \"r\") as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "# class LoadDicts:\n",
    "#     def __init__(self, dict_path: DictsPathType = \"./data\"):\n",
    "#         Dicts_glob = glob(f\"{dict_path}/*.json\")\n",
    "#         self.List = []\n",
    "#         self.Dict = {}\n",
    "#         for path_json in Dicts_glob:\n",
    "#             name = path_json.split(\"/\")[-1].replace(\".json\", \"\")\n",
    "#             self.List.append(name)\n",
    "#             self.Dict[name] = open_file_json(path_json)\n",
    "#             setattr(self, name, self.Dict[name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T01:43:44.383655Z",
     "iopub.status.busy": "2021-09-16T01:43:44.383446Z",
     "iopub.status.idle": "2021-09-16T01:43:45.851199Z",
     "shell.execute_reply": "2021-09-16T01:43:45.850385Z",
     "shell.execute_reply.started": "2021-09-16T01:43:44.383631Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.9.7\n",
      "IPython version      : 7.27.0\n",
      "\n",
      "Compiler    : GCC 10.2.1 20210110\n",
      "OS          : Linux\n",
      "Release     : 5.11.0-7620-generic\n",
      "Machine     : x86_64\n",
      "Processor   : \n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 6a780e1790410c6dfa628cd7e3ced687762803a5\n",
      "\n",
      "Git repo: https://github.com/ysraell/random-forest-mc.git\n",
      "\n",
      "Git branch: main\n",
      "\n",
      "numpy : 1.20.3\n",
      "pandas: 1.3.2\n",
      "joblib: 1.0.1\n",
      "\n",
      "CPU\t: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz\n",
      "Mem:            31G\n",
      "Swap:          4.0G\n"
     ]
    }
   ],
   "source": [
    "# Run this cell before close.\n",
    "%watermark -d --iversion -b -r -g -m -v\n",
    "!cat /proc/cpuinfo |grep 'model name'|head -n 1 |sed -e 's/model\\ name/CPU/'\n",
    "!free -h |cut -d'i' -f1  |grep -v total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T01:43:45.852485Z",
     "iopub.status.busy": "2021-09-16T01:43:45.852291Z",
     "iopub.status.idle": "2021-09-16T01:43:47.296080Z",
     "shell.execute_reply": "2021-09-16T01:43:47.295561Z",
     "shell.execute_reply.started": "2021-09-16T01:43:45.852457Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/work/data/creditcard_trans_int.csv')\n",
    "target_col = 'Class'\n",
    "ds_cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
    "df[target_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T01:44:19.459528Z",
     "iopub.status.busy": "2021-09-16T01:44:19.459086Z",
     "iopub.status.idle": "2021-09-16T01:44:19.464733Z",
     "shell.execute_reply": "2021-09-16T01:44:19.464121Z",
     "shell.execute_reply.started": "2021-09-16T01:44:19.459483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T01:44:47.435311Z",
     "iopub.status.busy": "2021-09-16T01:44:47.434323Z",
     "iopub.status.idle": "2021-09-16T01:44:47.458460Z",
     "shell.execute_reply": "2021-09-16T01:44:47.457073Z",
     "shell.execute_reply.started": "2021-09-16T01:44:47.435245Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How much for test step:\n",
    "N_fraud_test = 200\n",
    "N_truth_test = int(2e4)\n",
    "\n",
    "# How much for training step:\n",
    "N_truth_train = int(2e5)\n",
    "# The remaing data for fraud is all used.\n",
    "\n",
    "# How much (survived) trees:\n",
    "T = 32\n",
    "\n",
    "# Parameters for bootstrap\n",
    "\n",
    "# Data for each tree:\n",
    "N_T = int((df[target_col].value_counts()[1] - N_fraud_test)/2) # remaing data for fraud.\n",
    "N_V = N_T # keep balanced amount for each classe.\n",
    "\n",
    "# How much features:\n",
    "n_F_max = len(ds_cols)\n",
    "n_F_min = 20\n",
    "# From EDA: decision trees with small amount of features could be not so efficient.\n",
    "\n",
    "# Droped trees limit:\n",
    "D = 256\n",
    "# When a total of D trees are dropped, the validation threshold decreases delta_Th, restarting the counting.\n",
    "\n",
    "# Delta Threshold\n",
    "delta_th = 0.001\n",
    "th_start = 0.999\n",
    "get_best_tree = True\n",
    "# The validation threshold decreases dynamically as need to selection more specialized tree.\n",
    "\n",
    "# Metric in validation\n",
    "M = 'Accuracy'\n",
    "# This parameter have no effect, only to say which metric is used in validation process.\n",
    "\n",
    "#Seeds\n",
    "#split_seeds = [43, 47, 53, 59]\n",
    "split_seeds = [43]\n",
    "# One for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T01:45:04.734708Z",
     "iopub.status.busy": "2021-09-16T01:45:04.733982Z",
     "iopub.status.idle": "2021-09-16T02:07:04.355327Z",
     "shell.execute_reply": "2021-09-16T02:07:04.354734Z",
     "shell.execute_reply.started": "2021-09-16T01:45:04.734629Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8350954853d4ae2b26c671f7a80a556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Planting the forest:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splitting by unique values of the classes.\n",
    "df = df[ds_cols+[target_col]]\n",
    "df_fraud = df.query('Class == 1').reset_index(drop=True).copy()\n",
    "df_truth = df.query('Class == 0').reset_index(drop=True).copy()\n",
    "\n",
    "Results = {}\n",
    "    \n",
    "# Each seed is a experiment.\n",
    "for seed in split_seeds:\n",
    "\n",
    "    # Start the experiment\n",
    "    df_fraud_train, df_fraud_test = train_test_split(df_fraud, test_size=N_fraud_test, random_state=seed)\n",
    "    df_truth_train, df_truth_test = train_test_split(df_truth, train_size=N_truth_train, test_size=N_truth_test, random_state=seed)\n",
    "    df_train = pd.concat([df_fraud_train, df_truth_train]).reset_index(drop=True)\n",
    "    df_test = pd.concat([df_fraud_test, df_truth_test]).reset_index(drop=True)\n",
    "    del df_fraud_test, df_truth_test, df_fraud_train, df_truth_train\n",
    "    \n",
    "    # Training step\n",
    "    cls = RandomForestMC(\n",
    "        n_trees=T, target_col='Class', max_discard_trees=D,\n",
    "        delta_th=delta_th, th_start=th_start,\n",
    "        batch_train_pclass=N_T, batch_val_pclass=N_V,\n",
    "        th_decease_verbose = False, get_best_tree=get_best_tree\n",
    "    )\n",
    "    cls.process_dataset(df_train)\n",
    "    cls.fitParallel(max_workers=16, thread_parallel_method=False)\n",
    "    #cls.fit()\n",
    "\n",
    "    modeldict = cls.model2dict()\n",
    "    dump_file_json('/work/data/cls_rfmc_v033_2.json', modeldict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T02:07:04.356743Z",
     "iopub.status.busy": "2021-09-16T02:07:04.356575Z",
     "iopub.status.idle": "2021-09-16T02:07:28.539318Z",
     "shell.execute_reply": "2021-09-16T02:07:28.538747Z",
     "shell.execute_reply.started": "2021-09-16T02:07:04.356718Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "    # Test step\n",
    "    probs = cls.testForestProbs(df_test)\n",
    "\n",
    "\n",
    "    # Save results\n",
    "    target_test = df_test.Class.to_list()\n",
    "    Results[seed] = {\n",
    "        'probs': probs,\n",
    "        'target_test': target_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T02:07:28.540331Z",
     "iopub.status.busy": "2021-09-16T02:07:28.540069Z",
     "iopub.status.idle": "2021-09-16T02:07:30.066887Z",
     "shell.execute_reply": "2021-09-16T02:07:30.066343Z",
     "shell.execute_reply.started": "2021-09-16T02:07:28.540307Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generates the metrics\n",
    "data = []\n",
    "for seed,exp in Results.items():\n",
    "    target_test = exp['target_test']\n",
    "    rf_classes = [0, 1]\n",
    "    df_exp = pd.DataFrame([(d['0'], d['1']) for d in exp['probs']], columns=rf_classes)\n",
    "    df_exp['pred'] = df_exp[[0, 1]].apply(lambda x: rf_classes[np.argmax(x)], axis=1)\n",
    "    df_exp['target'] = target_test\n",
    "    Fraud_True_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)][1].sum()/sum(df_exp.target == 1)\n",
    "    Truth_False_Sum = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 1)][0].sum()/sum(df_exp.target == 1)\n",
    "    Fraud_False_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 0)][1].sum()/sum(df_exp.target == 0)\n",
    "    F1_M = f1_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    AUC_ROC_M = roc_auc_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    TP_0 = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 0)].shape[0]/sum(df_exp.target == 0)\n",
    "    TP_1 = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)].shape[0]/sum(df_exp.target == 1)\n",
    "\n",
    "\n",
    "    data.append([\n",
    "        seed, Fraud_True_Sum, Truth_False_Sum, Fraud_False_Sum, F1_M, AUC_ROC_M, TP_0, TP_1\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T02:07:30.068489Z",
     "iopub.status.busy": "2021-09-16T02:07:30.068276Z",
     "iopub.status.idle": "2021-09-16T02:07:30.088124Z",
     "shell.execute_reply": "2021-09-16T02:07:30.087467Z",
     "shell.execute_reply.started": "2021-09-16T02:07:30.068465Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>Fraud_True_Sum</th>\n",
       "      <th>Truth_False_Sum</th>\n",
       "      <th>Fraud_False_Sum</th>\n",
       "      <th>F1_M</th>\n",
       "      <th>AUC_ROC_M</th>\n",
       "      <th>TP_0</th>\n",
       "      <th>TP_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>0.8325</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>0.007073</td>\n",
       "      <td>0.787629</td>\n",
       "      <td>0.92695</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  Fraud_True_Sum  Truth_False_Sum  Fraud_False_Sum      F1_M  \\\n",
       "0    43          0.8325           0.1075         0.007073  0.787629   \n",
       "\n",
       "   AUC_ROC_M    TP_0   TP_1  \n",
       "0    0.92695  0.9889  0.865  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['seed', 'Fraud_True_Sum','Truth_False_Sum', 'Fraud_False_Sum', 'F1_M', 'AUC_ROC_M', 'TP_0', 'TP_1']\n",
    "df_Results = pd.DataFrame(data, columns=columns)\n",
    "df_Results.to_csv('/work/data/Results_creditcard_RFMC_v033_3.csv', index=False)\n",
    "df_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T02:07:30.089497Z",
     "iopub.status.busy": "2021-09-16T02:07:30.089189Z",
     "iopub.status.idle": "2021-09-16T02:07:30.094880Z",
     "shell.execute_reply": "2021-09-16T02:07:30.094361Z",
     "shell.execute_reply.started": "2021-09-16T02:07:30.089461Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "0.9126 (0.8904 / 0.9281)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "md(\"{:.4f} ({:.4f} / {:.4f})\".format(sum(cls.survived_scores)/len(cls.survived_scores), min(cls.survived_scores), max(cls.survived_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T02:07:30.096159Z",
     "iopub.status.busy": "2021-09-16T02:07:30.095754Z",
     "iopub.status.idle": "2021-09-16T02:32:10.618522Z",
     "shell.execute_reply": "2021-09-16T02:32:10.617907Z",
     "shell.execute_reply.started": "2021-09-16T02:07:30.096132Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4193a560624343c08ad717abc52bb1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Planting the forest:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "0.9123 (0.8904 / 0.9315)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>Fraud_True_Sum</th>\n",
       "      <th>Truth_False_Sum</th>\n",
       "      <th>Fraud_False_Sum</th>\n",
       "      <th>F1_M</th>\n",
       "      <th>AUC_ROC_M</th>\n",
       "      <th>TP_0</th>\n",
       "      <th>TP_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>0.824922</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>0.800552</td>\n",
       "      <td>0.925125</td>\n",
       "      <td>0.99025</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  Fraud_True_Sum  Truth_False_Sum  Fraud_False_Sum      F1_M  \\\n",
       "0    43        0.824922         0.109375         0.006041  0.800552   \n",
       "\n",
       "   AUC_ROC_M     TP_0  TP_1  \n",
       "0   0.925125  0.99025  0.86  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cls.max_discard_trees = D\n",
    "cls.fitParallel(max_workers=16, thread_parallel_method=False)\n",
    "modeldict = cls.model2dict()\n",
    "dump_file_json('/work/data/cls_rfmc_v033_2.json', modeldict)\n",
    "\n",
    "# Test step\n",
    "probs = cls.testForestProbs(df_test)\n",
    "\n",
    "# Save results\n",
    "target_test = df_test.Class.to_list()\n",
    "Results[seed] = {\n",
    "    'probs': probs,\n",
    "    'target_test': target_test\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Generates the metrics\n",
    "data = []\n",
    "for seed,exp in Results.items():\n",
    "    target_test = exp['target_test']\n",
    "    rf_classes = [0, 1]\n",
    "    df_exp = pd.DataFrame([(d['0'], d['1']) for d in exp['probs']], columns=rf_classes)\n",
    "    df_exp['pred'] = df_exp[[0, 1]].apply(lambda x: rf_classes[np.argmax(x)], axis=1)\n",
    "    df_exp['target'] = target_test\n",
    "    Fraud_True_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)][1].sum()/sum(df_exp.target == 1)\n",
    "    Truth_False_Sum = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 1)][0].sum()/sum(df_exp.target == 1)\n",
    "    Fraud_False_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 0)][1].sum()/sum(df_exp.target == 0)\n",
    "    F1_M = f1_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    AUC_ROC_M = roc_auc_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    TP_0 = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 0)].shape[0]/sum(df_exp.target == 0)\n",
    "    TP_1 = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)].shape[0]/sum(df_exp.target == 1)\n",
    "\n",
    "\n",
    "    data.append([\n",
    "        seed, Fraud_True_Sum, Truth_False_Sum, Fraud_False_Sum, F1_M, AUC_ROC_M, TP_0, TP_1\n",
    "    ])\n",
    "    \n",
    "columns = ['seed', 'Fraud_True_Sum','Truth_False_Sum', 'Fraud_False_Sum', 'F1_M', 'AUC_ROC_M', 'TP_0', 'TP_1']\n",
    "df_Results = pd.DataFrame(data, columns=columns)\n",
    "md(\"{:.4f} ({:.4f} / {:.4f})\".format(sum(cls.survived_scores)/len(cls.survived_scores), min(cls.survived_scores), max(cls.survived_scores)))\n",
    "display(df_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T02:32:10.620067Z",
     "iopub.status.busy": "2021-09-16T02:32:10.619540Z",
     "iopub.status.idle": "2021-09-16T02:56:33.434179Z",
     "shell.execute_reply": "2021-09-16T02:56:33.433609Z",
     "shell.execute_reply.started": "2021-09-16T02:32:10.620022Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ded9a39fe24bb1a6e972c6358523e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Planting the forest:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "0.9125 (0.8904 / 0.9315)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>Fraud_True_Sum</th>\n",
       "      <th>Truth_False_Sum</th>\n",
       "      <th>Fraud_False_Sum</th>\n",
       "      <th>F1_M</th>\n",
       "      <th>AUC_ROC_M</th>\n",
       "      <th>TP_0</th>\n",
       "      <th>TP_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>0.824167</td>\n",
       "      <td>0.108854</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>0.800005</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seed  Fraud_True_Sum  Truth_False_Sum  Fraud_False_Sum      F1_M  \\\n",
       "0    43        0.824167         0.108854         0.006072  0.800005   \n",
       "\n",
       "   AUC_ROC_M    TP_0  TP_1  \n",
       "0     0.9251  0.9902  0.86  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cls.max_discard_trees = D\n",
    "cls.fitParallel(max_workers=16, thread_parallel_method=False)\n",
    "modeldict = cls.model2dict()\n",
    "dump_file_json('/work/data/cls_rfmc_v033_2.json', modeldict)\n",
    "\n",
    "# Test step\n",
    "probs = cls.testForestProbs(df_test)\n",
    "\n",
    "# Save results\n",
    "target_test = df_test.Class.to_list()\n",
    "Results[seed] = {\n",
    "    'probs': probs,\n",
    "    'target_test': target_test\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Generates the metrics\n",
    "data = []\n",
    "for seed,exp in Results.items():\n",
    "    target_test = exp['target_test']\n",
    "    rf_classes = [0, 1]\n",
    "    df_exp = pd.DataFrame([(d['0'], d['1']) for d in exp['probs']], columns=rf_classes)\n",
    "    df_exp['pred'] = df_exp[[0, 1]].apply(lambda x: rf_classes[np.argmax(x)], axis=1)\n",
    "    df_exp['target'] = target_test\n",
    "    Fraud_True_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)][1].sum()/sum(df_exp.target == 1)\n",
    "    Truth_False_Sum = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 1)][0].sum()/sum(df_exp.target == 1)\n",
    "    Fraud_False_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 0)][1].sum()/sum(df_exp.target == 0)\n",
    "    F1_M = f1_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    AUC_ROC_M = roc_auc_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    TP_0 = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 0)].shape[0]/sum(df_exp.target == 0)\n",
    "    TP_1 = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)].shape[0]/sum(df_exp.target == 1)\n",
    "\n",
    "\n",
    "    data.append([\n",
    "        seed, Fraud_True_Sum, Truth_False_Sum, Fraud_False_Sum, F1_M, AUC_ROC_M, TP_0, TP_1\n",
    "    ])\n",
    "    \n",
    "columns = ['seed', 'Fraud_True_Sum','Truth_False_Sum', 'Fraud_False_Sum', 'F1_M', 'AUC_ROC_M', 'TP_0', 'TP_1']\n",
    "df_Results = pd.DataFrame(data, columns=columns)\n",
    "md(\"{:.4f} ({:.4f} / {:.4f})\".format(sum(cls.survived_scores)/len(cls.survived_scores), min(cls.survived_scores), max(cls.survived_scores)))\n",
    "display(df_Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-16T02:56:33.435494Z",
     "iopub.status.busy": "2021-09-16T02:56:33.435129Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cd4a0de58043c0825d15c2a6f9e4d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Planting the forest:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cls.max_discard_trees = D\n",
    "cls.fitParallel(max_workers=16, thread_parallel_method=False)\n",
    "modeldict = cls.model2dict()\n",
    "dump_file_json('/work/data/cls_rfmc_v033_2.json', modeldict)\n",
    "\n",
    "# Test step\n",
    "probs = cls.testForestProbs(df_test)\n",
    "\n",
    "# Save results\n",
    "target_test = df_test.Class.to_list()\n",
    "Results[seed] = {\n",
    "    'probs': probs,\n",
    "    'target_test': target_test\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Generates the metrics\n",
    "data = []\n",
    "for seed,exp in Results.items():\n",
    "    target_test = exp['target_test']\n",
    "    rf_classes = [0, 1]\n",
    "    df_exp = pd.DataFrame([(d['0'], d['1']) for d in exp['probs']], columns=rf_classes)\n",
    "    df_exp['pred'] = df_exp[[0, 1]].apply(lambda x: rf_classes[np.argmax(x)], axis=1)\n",
    "    df_exp['target'] = target_test\n",
    "    Fraud_True_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)][1].sum()/sum(df_exp.target == 1)\n",
    "    Truth_False_Sum = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 1)][0].sum()/sum(df_exp.target == 1)\n",
    "    Fraud_False_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 0)][1].sum()/sum(df_exp.target == 0)\n",
    "    F1_M = f1_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    AUC_ROC_M = roc_auc_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    TP_0 = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 0)].shape[0]/sum(df_exp.target == 0)\n",
    "    TP_1 = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)].shape[0]/sum(df_exp.target == 1)\n",
    "\n",
    "\n",
    "    data.append([\n",
    "        seed, Fraud_True_Sum, Truth_False_Sum, Fraud_False_Sum, F1_M, AUC_ROC_M, TP_0, TP_1\n",
    "    ])\n",
    "    \n",
    "columns = ['seed', 'Fraud_True_Sum','Truth_False_Sum', 'Fraud_False_Sum', 'F1_M', 'AUC_ROC_M', 'TP_0', 'TP_1']\n",
    "df_Results = pd.DataFrame(data, columns=columns)\n",
    "md(\"{:.4f} ({:.4f} / {:.4f})\".format(sum(cls.survived_scores)/len(cls.survived_scores), min(cls.survived_scores), max(cls.survived_scores)))\n",
    "display(df_Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test step\n",
    "cls.setSoftVoting(True)\n",
    "cls.testForestProbs(df_test)\n",
    "\n",
    "# Save results\n",
    "target_test = df_test.Class.to_list()\n",
    "Results[seed] = {\n",
    "    'probs': probs,\n",
    "    'target_test': target_test\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Generates the metrics\n",
    "data = []\n",
    "for seed,exp in Results.items():\n",
    "    target_test = exp['target_test']\n",
    "    rf_classes = [0, 1]\n",
    "    df_exp = pd.DataFrame([(d['0'], d['1']) for d in exp['probs']], columns=rf_classes)\n",
    "    df_exp['pred'] = df_exp[[0, 1]].apply(lambda x: rf_classes[np.argmax(x)], axis=1)\n",
    "    df_exp['target'] = target_test\n",
    "    Fraud_True_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)][1].sum()/sum(df_exp.target == 1)\n",
    "    Truth_False_Sum = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 1)][0].sum()/sum(df_exp.target == 1)\n",
    "    Fraud_False_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 0)][1].sum()/sum(df_exp.target == 0)\n",
    "    F1_M = f1_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    AUC_ROC_M = roc_auc_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    TP_0 = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 0)].shape[0]/sum(df_exp.target == 0)\n",
    "    TP_1 = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)].shape[0]/sum(df_exp.target == 1)\n",
    "\n",
    "\n",
    "    data.append([\n",
    "        seed, Fraud_True_Sum, Truth_False_Sum, Fraud_False_Sum, F1_M, AUC_ROC_M, TP_0, TP_1\n",
    "    ])\n",
    "    \n",
    "columns = ['seed', 'Fraud_True_Sum','Truth_False_Sum', 'Fraud_False_Sum', 'F1_M', 'AUC_ROC_M', 'TP_0', 'TP_1']\n",
    "df_Results = pd.DataFrame(data, columns=columns)\n",
    "md(\"{:.4f} ({:.4f} / {:.4f})\".format(sum(cls.survived_scores)/len(cls.survived_scores), min(cls.survived_scores), max(cls.survived_scores)))\n",
    "display(df_Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test step\n",
    "cls.setSoftVoting(False)\n",
    "cls.setWeightedTrees(True)\n",
    "probs = cls.testForestProbs(df_test)\n",
    "\n",
    "# Save results\n",
    "target_test = df_test.Class.to_list()\n",
    "Results[seed] = {\n",
    "    'probs': probs,\n",
    "    'target_test': target_test\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Generates the metrics\n",
    "data = []\n",
    "for seed,exp in Results.items():\n",
    "    target_test = exp['target_test']\n",
    "    rf_classes = [0, 1]\n",
    "    df_exp = pd.DataFrame([(d['0'], d['1']) for d in exp['probs']], columns=rf_classes)\n",
    "    df_exp['pred'] = df_exp[[0, 1]].apply(lambda x: rf_classes[np.argmax(x)], axis=1)\n",
    "    df_exp['target'] = target_test\n",
    "    Fraud_True_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)][1].sum()/sum(df_exp.target == 1)\n",
    "    Truth_False_Sum = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 1)][0].sum()/sum(df_exp.target == 1)\n",
    "    Fraud_False_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 0)][1].sum()/sum(df_exp.target == 0)\n",
    "    F1_M = f1_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    AUC_ROC_M = roc_auc_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    TP_0 = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 0)].shape[0]/sum(df_exp.target == 0)\n",
    "    TP_1 = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)].shape[0]/sum(df_exp.target == 1)\n",
    "\n",
    "\n",
    "    data.append([\n",
    "        seed, Fraud_True_Sum, Truth_False_Sum, Fraud_False_Sum, F1_M, AUC_ROC_M, TP_0, TP_1\n",
    "    ])\n",
    "    \n",
    "columns = ['seed', 'Fraud_True_Sum','Truth_False_Sum', 'Fraud_False_Sum', 'F1_M', 'AUC_ROC_M', 'TP_0', 'TP_1']\n",
    "df_Results = pd.DataFrame(data, columns=columns)\n",
    "md(\"{:.4f} ({:.4f} / {:.4f})\".format(sum(cls.survived_scores)/len(cls.survived_scores), min(cls.survived_scores), max(cls.survived_scores)))\n",
    "display(df_Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test step\n",
    "cls.setSoftVoting(True)\n",
    "cls.setWeightedTrees(True)\n",
    "probs = cls.testForestProbs(df_test)\n",
    "\n",
    "# Save results\n",
    "target_test = df_test.Class.to_list()\n",
    "Results[seed] = {\n",
    "    'probs': probs,\n",
    "    'target_test': target_test\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Generates the metrics\n",
    "data = []\n",
    "for seed,exp in Results.items():\n",
    "    target_test = exp['target_test']\n",
    "    rf_classes = [0, 1]\n",
    "    df_exp = pd.DataFrame([(d['0'], d['1']) for d in exp['probs']], columns=rf_classes)\n",
    "    df_exp['pred'] = df_exp[[0, 1]].apply(lambda x: rf_classes[np.argmax(x)], axis=1)\n",
    "    df_exp['target'] = target_test\n",
    "    Fraud_True_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)][1].sum()/sum(df_exp.target == 1)\n",
    "    Truth_False_Sum = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 1)][0].sum()/sum(df_exp.target == 1)\n",
    "    Fraud_False_Sum = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 0)][1].sum()/sum(df_exp.target == 0)\n",
    "    F1_M = f1_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    AUC_ROC_M = roc_auc_score(target_test, df_exp['pred'].to_numpy(), average='macro')\n",
    "    TP_0 = df_exp.loc[(df_exp.pred == 0) & (df_exp.target == 0)].shape[0]/sum(df_exp.target == 0)\n",
    "    TP_1 = df_exp.loc[(df_exp.pred == 1) & (df_exp.target == 1)].shape[0]/sum(df_exp.target == 1)\n",
    "\n",
    "\n",
    "    data.append([\n",
    "        seed, Fraud_True_Sum, Truth_False_Sum, Fraud_False_Sum, F1_M, AUC_ROC_M, TP_0, TP_1\n",
    "    ])\n",
    "    \n",
    "columns = ['seed', 'Fraud_True_Sum','Truth_False_Sum', 'Fraud_False_Sum', 'F1_M', 'AUC_ROC_M', 'TP_0', 'TP_1']\n",
    "df_Results = pd.DataFrame(data, columns=columns)\n",
    "md(\"{:.4f} ({:.4f} / {:.4f})\".format(sum(cls.survived_scores)/len(cls.survived_scores), min(cls.survived_scores), max(cls.survived_scores)))\n",
    "display(df_Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Results.to_csv('/work/data/Results_creditcard_RFMC_v033_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-09-14T03:07:41.534305Z",
     "iopub.status.idle": "2021-09-14T03:07:41.534729Z",
     "shell.execute_reply": "2021-09-14T03:07:41.534545Z",
     "shell.execute_reply.started": "2021-09-14T03:07:41.534524Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Results = pd.read_csv('/work/data/Results_creditcard_RFMC_V030_4.csv')\n",
    "df_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
